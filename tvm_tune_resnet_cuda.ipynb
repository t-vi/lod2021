{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import IPython\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Auto-Optimization with TVM\n",
    "\n",
    "This is mostly: Auto-scheduling a Neural Network for NVIDIA GPU, Author: Lianmin Zheng\n",
    "and adapted to a PyTorch model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "import tvm\n",
    "from tvm import relay, auto_scheduler\n",
    "import tvm.relay.testing\n",
    "from tvm.contrib import graph_executor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define a Network\n",
    "\n",
    "We export a model in TorchScript and import into TVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchvision\n",
    "import torch\n",
    "model = torchvision.models.resnet18(pretrained=True).eval().cuda()\n",
    "input_shape = 1, 3, 224, 224\n",
    "inp = torch.randn(input_shape, device=\"cuda\")\n",
    "traced_model = torch.jit.trace(model, inp)\n",
    "output_shape = traced_model(inp).shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "target = tvm.target.Target(\"cuda\")\n",
    "mod, params = tvm.relay.frontend.from_pytorch(traced_model, [('input', input_shape)])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Extract Search Tasks\n",
    "--------------------\n",
    "Next, we extract the search tasks and their weights from a network.\n",
    "The weight of a task is the number of appearances of the task's subgraph\n",
    "in the whole network.\n",
    "By using the weight, we can approximate the end-to-end latency of the network\n",
    "as :code:`sum(latency[t] * weight[t])`, where :code:`latency[t]` is the\n",
    "latency of a task and :code:`weight[t]` is the weight of the task.\n",
    "The task scheduler will just optimize this objective.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extract tasks...\n",
      "========== Task 0  (workload key: [\"ef46995b12a7cbd7bb7acd348966979c\", [1, 3, 224, 224], [64, 3, 7, 7], [1, 64, 1, 1], [1, 64, 112, 112]]) ==========\n",
      "========== Task 1  (workload key: [\"5a5d7a8547d34798d6f42c3bb7a23a25\", [1, 128, 28, 28], [128, 128, 3, 3], [1, 128, 1, 1], [1, 128, 28, 28]]) ==========\n",
      "========== Task 2  (workload key: [\"f09f3f33f76116d85452966717e745d5\", [1, 64, 112, 112], [1, 64, 56, 56]]) ==========\n",
      "========== Task 3  (workload key: [\"ef46995b12a7cbd7bb7acd348966979c\", [1, 256, 14, 14], [512, 256, 3, 3], [1, 512, 1, 1], [1, 512, 7, 7]]) ==========\n",
      "========== Task 4  (workload key: [\"11781f323b8c61af28b56f39334e38a5\", [1, 512, 7, 7], [1, 512, 1, 1]]) ==========\n",
      "========== Task 5  (workload key: [\"71e66151ca81542a8abc2f468939ab20\", [1, 128, 28, 28], [256, 128, 1, 1], [1, 256, 1, 1], [1, 256, 14, 14]]) ==========\n",
      "========== Task 6  (workload key: [\"6fb828dcd38877611a12b2d4bc756983\", [1, 64, 56, 56], [64, 64, 3, 3], [1, 64, 1, 1], [1, 64, 56, 56], [1, 64, 56, 56]]) ==========\n",
      "========== Task 7  (workload key: [\"6fb828dcd38877611a12b2d4bc756983\", [1, 128, 28, 28], [128, 128, 3, 3], [1, 128, 1, 1], [1, 128, 28, 28], [1, 128, 28, 28]]) ==========\n",
      "========== Task 8  (workload key: [\"ef46995b12a7cbd7bb7acd348966979c\", [1, 64, 56, 56], [128, 64, 3, 3], [1, 128, 1, 1], [1, 128, 28, 28]]) ==========\n",
      "========== Task 9  (workload key: [\"71e66151ca81542a8abc2f468939ab20\", [1, 64, 56, 56], [128, 64, 1, 1], [1, 128, 1, 1], [1, 128, 28, 28]]) ==========\n",
      "========== Task 10  (workload key: [\"71e66151ca81542a8abc2f468939ab20\", [1, 256, 14, 14], [512, 256, 1, 1], [1, 512, 1, 1], [1, 512, 7, 7]]) ==========\n",
      "========== Task 11  (workload key: [\"5a5d7a8547d34798d6f42c3bb7a23a25\", [1, 256, 14, 14], [256, 256, 3, 3], [1, 256, 1, 1], [1, 256, 14, 14]]) ==========\n",
      "========== Task 12  (workload key: [\"7d44c6e3c81cd80f61ff2265b2bae89a\", [1, 512], [1000, 512], [1, 1000], [1, 1000]]) ==========\n",
      "========== Task 13  (workload key: [\"6fb828dcd38877611a12b2d4bc756983\", [1, 256, 14, 14], [256, 256, 3, 3], [1, 256, 1, 1], [1, 256, 14, 14], [1, 256, 14, 14]]) ==========\n",
      "========== Task 14  (workload key: [\"ef46995b12a7cbd7bb7acd348966979c\", [1, 128, 28, 28], [256, 128, 3, 3], [1, 256, 1, 1], [1, 256, 14, 14]]) ==========\n",
      "========== Task 15  (workload key: [\"6fb828dcd38877611a12b2d4bc756983\", [1, 512, 7, 7], [512, 512, 3, 3], [1, 512, 1, 1], [1, 512, 7, 7], [1, 512, 7, 7]]) ==========\n",
      "========== Task 16  (workload key: [\"5a5d7a8547d34798d6f42c3bb7a23a25\", [1, 512, 7, 7], [512, 512, 3, 3], [1, 512, 1, 1], [1, 512, 7, 7]]) ==========\n",
      "========== Task 17  (workload key: [\"5a5d7a8547d34798d6f42c3bb7a23a25\", [1, 64, 56, 56], [64, 64, 3, 3], [1, 64, 1, 1], [1, 64, 56, 56]]) ==========\n"
     ]
    }
   ],
   "source": [
    "# Extract tasks from the network\n",
    "print(\"Extract tasks...\")\n",
    "#mod, params, input_shape, output_shape = get_network(network, batch_size, layout, dtype=dtype)\n",
    "tasks, task_weights = auto_scheduler.extract_tasks(mod[\"main\"], params, target)\n",
    "\n",
    "for idx, task in enumerate(tasks):\n",
    "    print(\"========== Task %d  (workload key: %s) ==========\" % (idx, task.workload_key))\n",
    "    #print(task.compute_dag)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Begin Tuning\n",
    "------------\n",
    "Now, we set some options for tuning and launch the search tasks\n",
    "\n",
    "* :code:`measure_ctx` launches a different process for measurement to\n",
    "  provide isolation. It can protect the master process from GPU crashes\n",
    "  during measurement and avoid other runtime conflicts.\n",
    "* :code:`min_repeat_ms` defines the minimum duration of one \"repeat\" in every measurement.\n",
    "  This can warmup the GPU, which is necessary to get accurate measurement results.\n",
    "  Typically, we recommend a value >= 300 ms.\n",
    "* :code:`num_measure_trials` is the number of measurement trials we can use during the tuning.\n",
    "  You can set it to a small number (e.g., 200) for a fast demonstrative run.\n",
    "  In practice, we recommend setting it around :code:`900 * len(tasks)`,\n",
    "  which is typically enough for the search to converge.\n",
    "  For example, there are 24 tasks in resnet-18, so we can set it as 20000.\n",
    "  You can adjust this parameter according to your time budget.\n",
    "* In addition, we use :code:`RecordToFile` to dump measurement records into a log file,\n",
    "  The measurement records can be used to query the history best, resume the search,\n",
    "  and do more analyses later.\n",
    "* see :any:`auto_scheduler.TuningOptions`,\n",
    "  :any:`auto_scheduler.LocalRPCMeasureContext` for more parameters.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "|  ID  | Latency (ms) | Speed (GFLOPS) | Trials |\n",
      "-------------------------------------------------\n",
      "|    0 |        0.041 |        5839.50 |      8 |\n",
      "|    1 |        0.112 |        2073.29 |      8 |\n",
      "|    2 |        0.004 |         504.65 |      8 |\n",
      "|    3 |        0.053 |        2177.95 |     16 |\n",
      "|    4 |        0.004 |          -0.00 |      8 |\n",
      "|    5 |        0.019 |         663.25 |      8 |\n",
      "|    6 |        0.052 |        4481.00 |      8 |\n",
      "|    7 |        0.052 |        4471.45 |     16 |\n",
      "|    8 |        0.038 |        3053.44 |      8 |\n",
      "|    9 |        0.016 |         798.09 |      8 |\n",
      "|   10 |        0.041 |         316.21 |      8 |\n",
      "|   11 |        0.119 |        1943.11 |      8 |\n",
      "|   12 |        0.018 |          56.89 |      8 |\n",
      "|   13 |        0.073 |        3162.12 |     16 |\n",
      "|   14 |        0.052 |        2226.61 |      8 |\n",
      "|   15 |        0.124 |        1859.29 |     24 |\n",
      "|   16 |        0.154 |        1498.07 |     16 |\n",
      "|   17 |        0.056 |        4156.56 |      8 |\n",
      "-------------------------------------------------\n",
      "Estimated total latency: 1.384 ms\tTrials: 192\tUsed time : 299 s\tNext ID: 11\t\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "class ClearOutput(auto_scheduler.task_scheduler.TaskSchedulerCallback):\n",
    "    def pre_tune(self, task_scheduler, task_id):\n",
    "        IPython.display.clear_output()\n",
    "\n",
    "log_file = 'tune.log'\n",
    "def run_tuning():\n",
    "    print(\"Begin tuning...\")\n",
    "    measure_ctx = auto_scheduler.LocalRPCMeasureContext(repeat=1, min_repeat_ms=300, timeout=10)\n",
    "\n",
    "    tuner = auto_scheduler.TaskScheduler(tasks, task_weights, callbacks=[\n",
    "        ClearOutput(),\n",
    "        tvm.auto_scheduler.task_scheduler.PrintTableInfo(),\n",
    "        tvm.auto_scheduler.task_scheduler.LogEstimatedLatency('total_latency.tsv')\n",
    "    ])\n",
    "    tune_option = auto_scheduler.TuningOptions(\n",
    "        num_measure_trials=200,  # change this to 20000 to achieve the best performance\n",
    "        num_measures_per_round=8, # 64\n",
    "        runner=measure_ctx.runner,\n",
    "        measure_callbacks=[auto_scheduler.RecordToFile(log_file)],\n",
    "    )\n",
    "\n",
    "    tuner.tune(tune_option)\n",
    "\n",
    "\n",
    "run_tuning()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compile and Evaluate\n",
    "--------------------\n",
    "After auto-tuning, we can compile the network with the best schedules we found.\n",
    "All measurement records are dumped into the log file during auto-tuning,\n",
    "so we can read the log file and load the best schedules.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Compile with the history best\n",
    "print(\"Compile...\")\n",
    "with auto_scheduler.ApplyHistoryBest(log_file):\n",
    "    with tvm.transform.PassContext(opt_level=3, config={\"relay.backend.use_auto_scheduler\": True}):\n",
    "        lib = relay.build(mod, target=target, params=params)\n",
    "# Create graph executor\n",
    "dev = tvm.device(str(target), 0)\n",
    "module = graph_executor.GraphModule(lib[\"default\"](dev))\n",
    "#data_tvm = tvm.nd.array((np.random.uniform(size=input_shape)).astype(dtype))\n",
    "import torch.utils.dlpack\n",
    "data_tvm = tvm.nd.from_dlpack(torch.utils.dlpack.to_dlpack(inp))\n",
    "\n",
    "module.set_input(\"input\", data_tvm)\n",
    "\n",
    "\n",
    "module.run(); dev.sync()\n",
    "%timeit module.run(); dev.sync()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.28 ms ± 161 ns per loop (mean ± std. dev. of 7 runs, 1000 loops each)\n"
     ]
    }
   ],
   "source": [
    "traced_model(inp); torch.cuda.synchronize()\n",
    "%timeit traced_model(inp); torch.cuda.synchronize()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Other Tips\n",
    "----------\n",
    "1. During the tuning, the auto-scheduler needs to compile many programs and\n",
    "   extract feature from them. This part is CPU-intensive,\n",
    "   so a high-performance CPU with many cores is recommended for faster search.\n",
    "2. You can use :code:`python3 -m tvm.auto_scheduler.measure_record --mode distill -i log.json`\n",
    "   to distill the large log file and only save the best useful records.\n",
    "3. You can resume a search from the previous log file. You just need to\n",
    "   add a new argument :code:`load_log_file` when creating the task scheduler\n",
    "   in function :code:`run_tuning`. Say,\n",
    "   :code:`tuner = auto_scheduler.TaskScheduler(tasks, task_weights, load_log_file=log_file)`\n",
    "4. If you have multiple target GPUs, you can use all of them for measurements to\n",
    "   parallelize the measurements. Check this `section <tutorials-autotvm-scale-up-rpc-tracker>`\n",
    "   to learn how to use the RPC Tracker and RPC Server.\n",
    "   To use the RPC Tracker in auto-scheduler, replace the runner in :code:`TuningOptions`\n",
    "   with :any:`auto_scheduler.RPCRunner`.\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
